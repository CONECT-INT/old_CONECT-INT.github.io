<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>events | CONECT | Computational Neuroscience Center @ INT</title>
    <link>https://conect-int.github.io/tag/events/</link>
      <atom:link href="https://conect-int.github.io/tag/events/index.xml" rel="self" type="application/rss+xml" />
    <description>events</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 29 Mar 2024 15:00:00 +0000</lastBuildDate>
    <image>
      <url>https://conect-int.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>events</title>
      <link>https://conect-int.github.io/tag/events/</link>
    </image>
    
    <item>
      <title>2024-03-29 : INT-CONECT seminar by Joe MacInnes</title>
      <link>https://conect-int.github.io/talk/2024-03-29-int-conect-seminar-by-joe-macinnes/</link>
      <pubDate>Fri, 29 Mar 2024 15:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2024-03-29-int-conect-seminar-by-joe-macinnes/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Friday, March 29th &lt;em&gt;&lt;strong&gt;14:30 to 15:30&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Gastaut&lt;/em&gt; (TBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this INT/CONECT seminar, &lt;a href=&#34;https://www.swansea.ac.uk/staff/william.macinnes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr Joe MacInnes&lt;/a&gt; will present his modelling work on &amp;ldquo;&lt;strong&gt;Casting a wide (neural) net: models and simulations of eye movements and attention&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract : Eye movements are an excellent proxy for visual attention, and offer a rich source of behavioural data. Decades of neuroscience and experimental results have provided many interesting artefacts that hint at underlying attentional mechanisms. Models and simulations of attention allow us the opportunity to implement our best theories and test them against a wide variety of experimental and imaging results.  Simulations of human eye movements, for example, can predict where we allocate attention, the temporal distributions of saccades and even the presence of attentional artifacts like Inhibition of Return, errors, anticipations and even virtual TMS lesions.  This talk will cover a couple of recent models that simulate attention and eye movements using deep learning neural nets, spiking network layers and diffusion models to simulate aspects, quirks and mechanisms of human attention.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Joe has an interdisciplinary PhD from Dalhousie University in Canada combining computer science (graphics and machine learning) with psychology (eye movements and attention).  He has worked in psychology departments (University of Toronto, University of Aberdeen, and HSE University Moscow), computer science departments (Dalhousie University, Canada, St Mary’s University, Canada, and Swansea University Wales) and industry research (Data visualization and AI, Canada).  He is currently a senior lecturer in AI at Swansea University department of computer science.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2024-03-11 : CONECT seminar by Danny Burnham</title>
      <link>https://conect-int.github.io/talk/2024-03-11-conect-seminar-by-danny-burnham/</link>
      <pubDate>Mon, 11 Mar 2024 15:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2024-03-11-conect-seminar-by-danny-burnham/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Monday, March 11th &lt;em&gt;&lt;strong&gt;14:30 to 16:00&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Laurent Vinay&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://ion.uoregon.edu/about/person-page/277&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Danny Burnham&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Mice alternate between inference- and stimulus-bound strategies during probabilistic foraging&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract : Essential features of the world are often hidden and must be inferred by constructing internal models based on indirect evidence. During foraging, animals must continually choose between trying to exploit a depleting food source at their current location and leaving to explore a new source at the expense of costly travel epochs. In a deterministic environment, the optimal strategy is to leave the current site when the immediate rate of reward drops below the average rate - a stimulus-bound strategy, assigning each action a value that is updated based on its immediate outcome. This strategy, however, is not optimal in a realistic foraging scenario, where rewards are encountered probabilistically and the optimal strategy is inference-bound, requiring the animal to infer the hidden structure of the world. Motivated by recent studies showing that mice alternate between discrete strategies during perceptual decision-making, we test the hypothesis that mouse behavior during a probabilistic foraging task switches between inference- and stimulus-bound strategies within the same session. To this end, we developed a novel hidden Markov model with linear emissions (LM-HMM) to capture this switching dynamic. When applied to mice engaged in the task, the LM-HMM revealed that mice switch between distinct inference bound and stimulus bound strategies exhibiting varying impulsivities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Danny Burnham is a computational neuroscientist from the &lt;a href=&#34;https://ion.uoregon.edu/about/person-page/277&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Neuroscience at the university of Oregon&lt;/a&gt; interested in Artificial Neural Network models of learning and memory.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-10-02 : CONECT seminar by Lorenzo Fontolan</title>
      <link>https://conect-int.github.io/talk/2023-10-02-conect-seminar-by-lorenzo-fontolan/</link>
      <pubDate>Mon, 02 Oct 2023 15:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-10-02-conect-seminar-by-lorenzo-fontolan/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Monday &lt;em&gt;&lt;strong&gt;15:00 to 16:00&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Laurent Vinay&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://fontolanl.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lorenzo Fontolan&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Neural mechanisms of memory-guided motor learning&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract : TBA&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Lorenzo Fontolan is a computational neuroscientist interested in how neural interactions give rise to cognitive phenomena, how brain circuits change during learning, and how mental disorders disrupt communication pathways in the brain. Currently he is a CENTURI Group Leader at Aix-Marseille University in France.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-09-22: Coding Club x CONECT on Optuna</title>
      <link>https://conect-int.github.io/talk/2023-09-22-coding-club-x-conect-on-optuna/</link>
      <pubDate>Fri, 22 Sep 2023 11:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-09-22-coding-club-x-conect-on-optuna/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: On Fridays at &lt;em&gt;&lt;strong&gt;11.00am to 12.00am&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Laurent Vinay (INT)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the first occurrence of monthly sessions on advanced computational tools for neuroscience to occur during the &lt;a href=&#34;https://framateam.org/int-marseille/channels/coding-club&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coding club&lt;/a&gt; organized by the &lt;a href=&#34;https://www.int.univ-amu.fr/plateformes/nit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NIT&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This Friday 22th Sept, we will have a look into &lt;a href=&#34;https://optuna.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt;, which is a generic-purpose optimizer for hyperparameters. It combines a diversity of &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modern search algorithms&lt;/a&gt; to optimize models in a black-box fashion, namely defining hyperparameters to optimize and an objective/cost function. Check the &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorials&lt;/a&gt; for further information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;COME TO SHARE AND TEST RECENT COMPUTATIONAL TOOLS FOR NEUROMODELING, ANALYZING YOUR DATA AND MUCH MORE!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The general purpose of these sessions is in line with the Coding Club on promoting open science in our daily practice. The focus of the CONECT contributions is on advanced (e.g. python, R, etc.) computational packages of interest for the local neuroscientific community, both theoreticians and experimentalists. Methodological questions like how to model specific neuronal systems and fitting experimental data are also in the scope of these sessions, with again a focus on practical tools related to simulation and analysis.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    To CONECT members: Propose topics at &lt;a href=&#34;https://amubox.univ-amu.fr/f/1359219652&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coding Club x CONECT&lt;/a&gt;.
And check the Mattermost channel for the schedule.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-09-21 : CONECT seminar by Jason Eshraghian</title>
      <link>https://conect-int.github.io/talk/2023-09-21-conect-seminar-by-jason-eshraghian/</link>
      <pubDate>Thu, 21 Sep 2023 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-09-21-conect-seminar-by-jason-eshraghian/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Thursday &lt;em&gt;&lt;strong&gt;2:30pm to 4pm&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Laurent Vinay&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://ncg.ucsc.edu/jason-eshraghian-bio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jason Eshraghian&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Making spiking neural networks do useful things&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This presentation will dive into how spiking neural networks can be trained to accomplish practical engineering problems. We will provide an overview of the various learning rules that have emerged over the past several decades, along with a few large-scale applications we’ve achieved with spike-based computation. This involves our spike-based language model, SpikeGPT, and our open-source Python library that adopts gradient-based optimization into spike-based models, snnTorch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Jason K. Eshraghian is an Assistant Professor with the Department of Electrical and Computer Engineering, University of California, Santa Cruz and the maintainer of &lt;a href=&#34;https://github.com/jeshraghian/snntorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;snnTorch&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In addition, we had a master class in the morning on &lt;a href=&#34;https://snntorch.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;snnTorch&lt;/a&gt;, get the notebook (upon request).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2023-09-11: CONECT seminar by Taro Toyoizumi</title>
      <link>https://conect-int.github.io/talk/2023-09-11-conect-seminar-by-taro-toyoizumi/</link>
      <pubDate>Mon, 11 Sep 2023 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-09-11-conect-seminar-by-taro-toyoizumi/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Monday 11th Sept 2023 &lt;em&gt;&lt;strong&gt;2.30pm to 3.30pm&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle de cours 5, batiment principal Timone- (aile verte)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this CONECT seminar co-organized with Institut de Neurosciences des Systèmes (INS), &lt;a href=&#34;https://toyoizumilab.riken.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taro Toyoizumi&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Modeling the fluctuations and state-dependence of synaptic dynamics&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract: Adaptive behavior, crucial for thriving in complex environments, is believed to be enabled by activity-dependent synaptic plasticity within neural circuits. In the first part of this talk, I present how synaptic plasticity could be stabilized in the brain. Conventional models of Hebbian plasticity often facilitate connections between coincidentally active neurons and produce pathologically synchronous neural activity. I demonstrate that biologically observed intrinsic synaptic dynamics—activity-independent changes in synapses—can maintain a physiological distribution of synaptic strength and stabilize memory within neural networks. In the second part, I adopt a top-down approach to model synaptic plasticity. Viewing the brain as an efficient information-processing organ, I assume that synaptic weights are updated to transmit information between neurons efficiently. This theory provides insights into the distinct outcomes of synaptic plasticity observed during the up and down states of non-rapid eye movement sleep, thereby shedding light on how memory consolidation may be influenced by the states and spatial scale of slow waves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Taro Toyoizumi leads the Lab for Neural Computation and Adaptation at &lt;a href=&#34;https://cbs.riken.jp/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RIKEN Center for Brain Science&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-07-10 : CONECT seminar by Adrien Fois</title>
      <link>https://conect-int.github.io/talk/2023-07-10-conect-seminar-by-adrien-fois/</link>
      <pubDate>Mon, 10 Jul 2023 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-07-10-conect-seminar-by-adrien-fois/</guid>
      <description>&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://www.researchgate.net/profile/Adrien-Fois-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adrien Fois&lt;/a&gt; did present his recent work on &amp;ldquo;&lt;strong&gt;Plasticity and Temporal Coding in Spiking Neural Networks Applied to Representation Learning&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The brain is a highly efficient computational system, capable of delivering 600 petaFlops while consuming only 20 W of energy, comparable to that of a light bulb. Computation is based on neural impulses, involving information encoding in the form of spikes and learning based on these spikes. According to the dominant paradigm, information is encoded by the number of spikes. However, an alternative paradigm suggests that information is contained in the precise timing of the spikes, offering significant advantages in terms of energy efficiency and information transfer speed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;My work aims to extract representations from temporal codes using event-based learning rules that are both spatially and temporally local. In particular, I will present a learning model that learns representations not in synaptic weights, but in transmission delays, which inherently operate in the temporal domain. Learning delays prove to be particularly relevant for processing temporal codes and enable the activation of a key function of spiking neurons: the detection of temporal coincidences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Adrien Fois was a PhD Student at the Institute Lorrain de Recherche en Informatique et Ses Applications and now a Post-doc at INT.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-03-03 : INT seminar by Andrea Alamia</title>
      <link>https://conect-int.github.io/talk/2023-03-03-int-seminar-by-andrea-alamia/</link>
      <pubDate>Fri, 03 Mar 2023 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-03-03-int-seminar-by-andrea-alamia/</guid>
      <description>&lt;p&gt;During this INT seminar, &lt;a href=&#34;https://artipago.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrea Alamia&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Interpreting oscillations as travelling waves: the role of alpha-band oscillations in cognition&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this talk, I will present three studies that characterize oscillatory travelling waves in the framework of Predictive Coding. In the first study, I’ll introduce a simple model of the visual cortex based on predictive coding mechanisms, in which physiological communication delays between levels generate alpha-band rhythms. Interestingly, these oscillations propagate as traveling waves across levels, both forward (during visual stimulation) and backward (during rest). Remarkably, experimental EEG data matched the predictions of our model. The second study refines the results of the first one, demonstrating that the direction of propagation of alpha-band waves is task dependent. Specifically, forward waves (from occipital to frontal regions) prevail during visual processing, whereas backward waves (from frontal to occipital areas) occur predominantly without visual stimulation. The last study explores the effect of a powerful psychedelics drug, N,N, Dimethyltryptamine (DMT), on alpha-band oscillations, considering a model proposed in the literature based on Predictive Coding. Despite participants being in the eye-closed condition, DMT elicits a spatio-temporal pattern of cortical activation (i.e. travelling waves) similar to that produced by visual stimulation, in line with the predictions of the proposed model. Lastly, I’ll show some preliminary results about the role of oscillatory traveling waves in schizophrenic patients, interpreting the results in the light of Predictive Coding.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Andrea Alamia is a CNRS researcher at the Brain and Cognition Research Center (CerCo) in Toulouse (France).
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-01-05 : CONECT seminar by Guillaume Dumas</title>
      <link>https://conect-int.github.io/talk/2023-01-05-conect-seminar-by-guillaume-dumas/</link>
      <pubDate>Thu, 05 Jan 2023 16:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-01-05-conect-seminar-by-guillaume-dumas/</guid>
      <description>&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://www.extrospection.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guillaume Dumas&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Multilevel Development of Cognitive Abilities in an Artificial Neural Network&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several neuronal mechanisms have been proposed to account for the formation of cognitive abilities through postnatal interactions with the physical and socio-cultural environment. Here, we introduce a three-level computational model of information processing and acquisition of cognitive abilities. We propose minimal architectural requirements to build these levels and how the parameters affect their performance and relationships. The first sensorimotor level handles local nonconscious processing, here during a visual classification task. The second level or cognitive level globally integrates the information from multiple local processors via long-ranged connections and synthesizes it in a global, but still nonconscious manner. The third and cognitively highest level handles the information globally and consciously. It is based on the Global Neuronal Workspace (GNW) theory and is referred to as conscious level. We use trace and delay conditioning tasks to, respectively, challenge the second and third levels. Results first highlight the necessity of epigenesis through selection and stabilization of synapses at both local and global scales to allow the network to solve the first two tasks. At the global scale, dopamine appears necessary to properly provide credit assignment despite the temporal delay between perception and reward. At the third level, the presence of interneurons becomes necessary to maintain a self-sustained representation within the GNW in the absence of sensory input. Finally, while balanced spontaneous intrinsic activity facilitates epigenesis at both local and global scales, the balanced excitatory-inhibitory ratio increases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;More info: &lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.2201304119&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.pnas.org/doi/10.1073/pnas.2201304119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keywords: computational biology, dynamical systems, medical machine learning (ML), neuroscience, AI ethics&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Guillaume Dumas is an Associate Professor of Computational Psychiatry in the Faculty of Medicine at the Université de Montréal, and the Principle Investigator of the Precision Psychiatry and Social Physiology laboratory at the CHU Sainte-Justine Research Center. He holds the IVADO professorship for “AI in Mental Health”, and the FRQS J1 in “AI and Digital Health”.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2022-11-24 : CONECT seminar by Bruno Cessac</title>
      <link>https://conect-int.github.io/talk/2022-11-24-conect-seminar-by-bruno-cessac/</link>
      <pubDate>Thu, 24 Nov 2022 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-11-24-conect-seminar-by-bruno-cessac/</guid>
      <description>&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://team.inria.fr/biovision/bruno-cessac/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bruno Cessac&lt;/a&gt; did present his recent work on &amp;ldquo;&lt;strong&gt;Retinal processing: Insights from mathematical modelling&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The retina is the entrance of the visual system. Although based on common biophysical principles, the dynamics of retinal neurons are quite different from their cortical counterparts, raising interesting problems for modellers. In this paper, I address some mathematically stated questions in this spirit, discussing, in particular: (1) How could lateral amacrine cell connectivity shape the spatio-temporal spike response of retinal ganglion cells? (2) How could spatio-temporal stimuli correlations and retinal network dynamics shape the spike train correlations at the output of the retina? These questions are addressed, first, introducing a mathematically tractable model of the layered retina, integrating amacrine cells’ lateral connectivity and piecewise linear rectification, allowing for computing the retinal ganglion cells receptive field together with the voltage and spike correlations of retinal ganglion cells resulting from the amacrine cells networks. Then, I review some recent results showing how the concept of spatio-temporal Gibbs distributions and linear response theory can be used to characterize the collective spike response to a spatio-temporal stimulus of a set of retinal ganglion cells, coupled via effective interactions corresponding to the amacrine cells network. On these bases, I briefly discuss several potential consequences of these results at the cortical level.
Keywords: retinal network; visual system; spatio-temporal spike correlations; linear response; non stationarity&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    My research was initially modeling and analysis of large sized dynamical systems arising in various fields such as physics, biology, sociology, computers networks. I have worked on subjects such as self-organized criticality, linear response in chaotic systems, social networks, communications networks. My main interest concerns neuronal networks dynamics. I have developed methods combining dynamical systems theory, statistical physics and ergodic theory allowing to classify dynamics arising in canonical neuronal networks models like integrate and fire models or firing rate models. I have applied these methods for the study of synaptic and intrinsic plasticity, dynamical learning, spike coding, spike train statistics analysis, mean-field dynamics. I am now involved in developing models for the visual system, especially the retina, as well as numerical methods and software for neuroscientists.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2022-10-05 : CONECT workshop &#34;Higher-order interactions in brain networks: from theory to data analysis&#34;</title>
      <link>https://conect-int.github.io/talk/2022-10-05-conect-workshop-higher-order-interactions-in-brain-networks-from-theory-to-data-analysis/</link>
      <pubDate>Wed, 05 Oct 2022 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-10-05-conect-workshop-higher-order-interactions-in-brain-networks-from-theory-to-data-analysis/</guid>
      <description>&lt;p&gt;We are pleased to invite you to a mini-workshop organized as a scientific activity of the Computational Neuroscience Center (CONECT) of the INT focused on &lt;strong&gt;&amp;ldquo;Higher-order interactions in brain networks: from theory to data analysis&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Two speakers will present their work from two complementary theoretical perspectives, emerged from recent advances in network science (Giovanni Petri) and information theory (Daniele Marinazzo).&lt;/p&gt;
&lt;p&gt;If you are interested in the topic and/or want to learn something new, please join us in the &lt;strong&gt;salle Laurent Vinay&lt;/strong&gt; on &lt;strong&gt;Wednesday 5th October 2022&lt;/strong&gt; at &lt;strong&gt;14:30&lt;/strong&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here are the titles and abstracts of the two presentations:&lt;/p&gt;
&lt;h2 id=&#34;giovanni-petri--turin-univ&#34;&gt;Giovanni Petri  (Turin Univ)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lordgrilo.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lordgrilo.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.co.uk/citations?user=jb__2PIAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scholar.google.co.uk/citations?user=jb__2PIAAAAJ&amp;hl=en&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Title: &lt;strong&gt;Between higher-order mechanisms and phenomena&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract: Complex networks have become the main paradigm for modelling the dynamics of complex interacting systems. However, networks are intrinsically limited to describing pairwise interactions, whereas real-world systems are often characterized by higher-order interactions involving groups of three or more units. Higher-order structures, such as hypergraphs and simplicial complexes, are therefore a better tool to map the real organization of many social, biological and man-made systems. At the same time, higher-order observables, typically topological or information-theoretic in nature and often sharing the same simplicial language, have been gathering  attention for their capacity to capture properties of complex systems that are invisible to standard statistical descriptions. This had led to a certain confusion between these two facets, mechanisms on one side, phenomena on the other. Here, using recent examples from both computational modeling and neuroimaging analysis, I highlight collective behaviours induced by higher-order interactions, their interface with recent advances in topological data analysis, and finally outline three key challenges for the physics of higher-order complex systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;daniele-marinazzo-ghent-univ&#34;&gt;Daniele Marinazzo (Ghent Univ)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://users.ugent.be/~dmarinaz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://users.ugent.be/~dmarinaz/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=OJbWSLoAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scholar.google.com/citations?user=OJbWSLoAAAAJ&amp;hl=en&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Title: Two  is company, three is a party, and how not to get lost in big parties: practical considerations on higher-order data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract: I will make the case for looking for high-order interactions in (neural) data, and for trying to do so in a principled yet feasible way (who needs yet another axiom?).
I will then loop back and show how we can find low-order descriptors of high-order interactions (we&amp;rsquo;re not good in figuring things in high dimensions).&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2022-09-08 : CONECT seminar by Charlie Sexton</title>
      <link>https://conect-int.github.io/talk/2022-09-08-conect-seminar-by-charlie-sexton/</link>
      <pubDate>Thu, 08 Sep 2022 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-09-08-conect-seminar-by-charlie-sexton/</guid>
      <description>&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://psychologicalsciences.unimelb.edu.au/research/msps-research-groups/timing/lab/people#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Charlie Sexton&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Spike-timing dependent plasticity among multiple layers of motion-sensitive neurons: a feedforward mechanism for motion extrapolation&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ability of the brain to represent the external world in real-time is impacted by the fact that neural processing takes time. Because neural delays accumulate as information progresses through the visual system, representations encoded at each hierarchical level are based upon input that is progressively outdated with respect to the external world. This is particularly relevant to the task of localizing a moving object – because the object’s location changes with time, neural representations of its location potentially lag behind its true location. It has therefore been proposed that the visual system utilizes the predictive nature of motion to extrapolate moving objects along their trajectory. Burkitt and Hogendoorn (2021, &lt;a href=&#34;https://doi.org/10.1523/JNEUROSCI.2017-20.2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1523/JNEUROSCI.2017-20.2021&lt;/a&gt;) showed how spike-timing dependent plasticity (STDP) can achieve motion extrapolation in a two-layer, feedforward network of velocity-tuned neurons, by shifting the receptive-fields of second-layer neurons in the opposite direction to a moving stimulus. The current study extends this work by implementing two important changes to the network to bring it more into line with biology: we expanded the network to multiple layers to reflect the depth of the visual hierarchy, and we implemented more realistic synaptic time-courses. We examine the degree to which STDP can facilitate compensation of neural delays across six layers, and show that the multi-layer network achieves cumulative compensation comparable in magnitude to the delays incurred in visual processing. We also explore the effect of additional delays imposed on the network by the integration time of the membrane potential.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    More about Charlie: So far in my PhD I have looked at how spike-timing dependent plasticity (STDP) can shift feedforward connection strengths between levels of the visual hierarchy, such that higher levels encode moving objects further along their trajectory (as a mechanism for motion extrapolation). The project began with a paper by my supervisors, Hinze Hogendoorn and Tony Burkitt: &lt;a href=&#34;https://www.jneurosci.org/content/41/20/4428.abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.jneurosci.org/content/41/20/4428.abstract&lt;/a&gt;. I have been working on extending this model network to have 6 layers to see the combined effect of plasticity at several visual areas. I will attach my abstract for my ECVP talk here.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2022-05-12 : A CONECT seminar &#34;Global organization of neuronal activity only requires unstructured local connectivity&#34; (David Dahmen)</title>
      <link>https://conect-int.github.io/talk/2022-05-12-a-conect-seminar-global-organization-of-neuronal-activity-only-requires-unstructured-local-connectivity-david-dahmen/</link>
      <pubDate>Thu, 12 May 2022 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-05-12-a-conect-seminar-global-organization-of-neuronal-activity-only-requires-unstructured-local-connectivity-david-dahmen/</guid>
      <description>&lt;p&gt;During this CONECT seminar, &lt;a href=&#34;https://scholar.google.com/citations?user=AZa4K5QAAAAJ&amp;amp;hl=fr&amp;amp;oi=ao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Dahmen&lt;/a&gt; did present his recent work on &amp;ldquo;&lt;strong&gt;Global organization of neuronal activity only requires unstructured local connectivity&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/35049496/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dahmen D, Layer M, Deutz L, Dąbrowska PA, Voges N, von Papen M, Brochier T, Riehle A, Diesmann M, Grün S, Helias M. Elife. 2022 Jan 20;11:e68422&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Dr. &lt;a href=&#34;https://www.fz-juelich.de/SharedDocs/Personen/INM/INM-6/EN/staff/Dahmen_David.html?nn=724620&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Dahmen&lt;/a&gt; is a PostDoc at the Research Centre Jülich (Institute of Neuroscience and Medicine (INM-6), Computational and Systems Neuroscience &amp;amp; Institute for Advanced Simulation (IAS-6), Theoretical Neuroscience &amp;amp; JARA-Institut Brain structure-function relationships (INM-10)).
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2022-03-04 : INT Seminar - &#34;Unifying Different Psychometric Methods : Theory and Experiment&#34; (Jonathan Vacher)</title>
      <link>https://conect-int.github.io/talk/2022-03-04-int-seminar-unifying-different-psychometric-methods-theory-and-experiment-jonathan-vacher/</link>
      <pubDate>Fri, 04 Mar 2022 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-03-04-int-seminar-unifying-different-psychometric-methods-theory-and-experiment-jonathan-vacher/</guid>
      <description>&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, &lt;a href=&#34;https://jonathanvacher.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Vacher&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Unifying Different Psychometric Methods : Theory and Experiment&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The two-alternative forced choice (2AFC) paradigm is one of the main methods used to measure perceptual thresholds and biases. Measurements from a 2AFC experiment can be modelled using signal detection theory (SDT) from which the psychometric function can be derived theoretically. Recent efforts to combine SDT with Bayesian probabilities has linked thresholds and biases to hypothesized prior knowledge and optimal encoding/decoding [1]. From another perspective, the maximum likelihood difference scaling (MLDS) paradigm is a more recent method that allows the experimenter to estimate a perceptual scale that links a physical property to a psychological dimension [2]. Such a perceptual scale is obtained from the comparison of relative differences between pairs of stimuli. Here again, the underlying model can be understood in terms of SDT and Bayesian probabilities. However, no comparison between MLDS and 2AFC measurements has been performed yet. Here, we introduce the theory that unifies those measurements and we present some preliminary experimental results. In this context, we further explore how MLDS measurements could help to understand the perception of more complex textures generated from the statistic of deep neural network features [3].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;[1] Wei, X. X., &amp;amp; Stocker, A. A. (2017). Lawful relation between perceptual bias and discriminability. Proceedings of the National Academy of Sciences, 114(38), 10244-10249.&lt;/p&gt;
&lt;p&gt;[2] Maloney, L. T., &amp;amp; Yang, J. N. (2003). Maximum likelihood difference scaling. Journal of Vision, 3(8):5, 573-585.&lt;/p&gt;
&lt;p&gt;[3] Vacher, J. &amp;amp; Davila, A., Kohn, A. &amp;amp; Coen-Cagli, R. (2021). Texture Interpolation for Probing Visual Perception. Advances in Neural Information Processing Systems, 33.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Dr. &lt;a href=&#34;https://jonathanvacher.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Vacher&lt;/a&gt; was a student at École Normale Supérieure de Cachan (now &lt;a href=&#34;http://www.ens-paris-saclay.fr/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saclay&lt;/a&gt;) where he pursued a degree in mathematics and applied mathematics. He completed his bachelor&amp;rsquo;s and master&amp;rsquo;s degree with a specialty in computational imaging and machine learning (&lt;a href=&#34;http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master MVA&lt;/a&gt;). He started a multidisciplinary PhD in mathematics (&lt;a href=&#34;http://www.dauphine.fr/en/research/research-centers/ceremade-umr-7534.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paris Dauphine University, CEREMADE)&lt;/a&gt; and neuroscience (&lt;a href=&#34;https://neuropsi.cnrs.fr/fr/icn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNRS, UNIC&lt;/a&gt;) under the supervision of &lt;a href=&#34;http://www.gpeyre.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gabriel Peyré&lt;/a&gt; and &lt;a href=&#34;https://www.unic.cnrs-gif.fr/people/cyril_monier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cyril Monier&lt;/a&gt;. Then, Jonathan was a postdoc at Albert Einstein College of Medicine in New-York between 2017 and 2020 while initiating a collaboration with Pascal Mamassian from the Laboratoire des Systèmes Perceptifs ()École Normale Supérieure de Paris) where he is currently a postdoc.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2022-02-24 : CONECT Seminar - &#34;Explainable AI for computational auditory neurosciences&#34; (Etienne Thoret)</title>
      <link>https://conect-int.github.io/talk/2022-02-24-conect-seminar-explainable-ai-for-computational-auditory-neurosciences-etienne-thoret/</link>
      <pubDate>Thu, 24 Feb 2022 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-02-24-conect-seminar-explainable-ai-for-computational-auditory-neurosciences-etienne-thoret/</guid>
      <description>&lt;p&gt;Etienne Thoret (ILCB/PRISM/LIS/AMU) kindly accepted to present his research project during our novel series of CONECT-core © seminars (=seminars open to all but focused on the core theoretical scientific questions of the CONECT members):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI for computational auditory neurosciences&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine learning and deep neural networks have been raised as compelling models to simulate a broad range of tasks on signals: from classification of sound events to the prediction of human physiological state from electrophysiological data. But what do we really understand about these models and how do they process the information they have been trained to process? As users, we often use them as tools without precisely understanding their mechanistic and representational underpinnings. In this talk, I&amp;rsquo;ll present recent works on how we can take part of these computational systems to answer fundamental research mysteries on auditory perception, speech production and cerebral processing. Beyond acoustics and sound perception, these techniques can find applications for the modeling of a variety of systems, including computational vision in robotics, haptics and clinical applications.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2021-12-10 : CONECT seminar - &#34;Sequence anticipation and STDP emerge from a voltage-based predictive learning rule&#34; (Matteo Saponati)</title>
      <link>https://conect-int.github.io/talk/2021-12-10-conect-seminar-sequence-anticipation-and-stdp-emerge-from-a-voltage-based-predictive-learning-rule-matteo-saponati/</link>
      <pubDate>Fri, 10 Dec 2021 11:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2021-12-10-conect-seminar-sequence-anticipation-and-stdp-emerge-from-a-voltage-based-predictive-learning-rule-matteo-saponati/</guid>
      <description>&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, &lt;a href=&#34;https://github.com/matteosaponati&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matteo Saponati&lt;/a&gt;, will present his recent work showing that &amp;ldquo;&lt;strong&gt;Sequence anticipation and STDP emerge from a voltage-based predictive learning rule&lt;/strong&gt;&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intelligent behavior depends on the brain’s ability to anticipate future events. However, the learning rules that enable neurons to predict and fire ahead of sensory inputs remain largely unknown. We propose a plasticity rule based on predictive processing, where the neuron learns a low-rank model of the synaptic input dynamics in its membrane potential. Neurons thereby amplify those synapses that maximally predict other synaptic inputs based on their temporal relations, which provide a solution to an optimization problem that can be implemented at the single-neuron level using only local information. Consequently, neurons learn sequences over long timescales and shift their spikes towards the first inputs in a sequence. We show that this mechanism can explain the development of anticipatory motion signaling and recall in the visual system. Furthermore, we demonstrate that the learning rule gives rise to several experimentally observed STDP (spike-timing-dependent plasticity) mechanisms. These findings suggest prediction as a guiding principle to orchestrate learning and synaptic plasticity in single neurons.
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.10.31.466667v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.biorxiv.org/content/10.1101/2021.10.31.466667v1&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;when: Friday 10th of December at 11am&lt;/li&gt;
&lt;li&gt;where: Salle Laurent Vinay at the &lt;a href=&#34;https://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Neurosciences Timone&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/matteosaponati&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matteo Saponati&lt;/a&gt; is a PhD candidate at &lt;a href=&#34;https://www.esi-frankfurt.de/research/vinck-lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ernst Strüngmann Institute (ESI) for Neuroscience - IMPRS for Neural Circuits&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2020-09-11 : CONECT seminar - &#34;Feedforward and feedback processes in visual recognition&#34; (T Serre)</title>
      <link>https://conect-int.github.io/talk/2020-09-11-conect-seminar-feedforward-and-feedback-processes-in-visual-recognition-t-serre/</link>
      <pubDate>Fri, 11 Sep 2020 14:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2020-09-11-conect-seminar-feedforward-and-feedback-processes-in-visual-recognition-t-serre/</guid>
      <description>&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, &lt;a href=&#34;http://serre-lab.clps.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Serre&lt;/a&gt; will present his recent work on &amp;ldquo;Feedforward and feedback processes in visual recognition&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching – and sometimes even surpassing – human accuracy on a variety of visual recognition tasks. In this talk, however, I will show that these neural networks and their recent extensions exhibit a limited ability to solve seemingly simple visual reasoning problems involving incremental grouping, similarity, and spatial relation judgments. Our group has developed a recurrent network model of classical and extra-classical receptive fields that is constrained by the anatomy and physiology of the visual cortex. The model was shown to account for diverse visual illusions providing computational evidence for a novel canonical circuit that is shared across visual modalities. I will show that this computational neuroscience model can be turned into a modern end-to-end trainable deep recurrent network architecture that addresses some of the shortcomings exhibited by state-of-the-art feedforward networks for solving complex visual reasoning tasks. This suggests that neuroscience may contribute powerful new ideas and approaches to computer science and artificial intelligence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Dr. &lt;a href=&#34;http://serre-lab.clps.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Serre&lt;/a&gt; is an Associate Professor in Cognitive Linguistic and Psychological Sciences and an affiliate of the Carney Institute for Brain Science at Brown University. He received a Ph.D. in Neuroscience from MIT in 2006 and an MSc in EECS from Télécom Bretagne (France) in 2000. His research seeks to understand the neural computations supporting visual perception and has been featured in the BBC series “Visions from the Future” and other news articles (The Economist, New Scientist, Scientific American, IEEE Computing in Science and Technology, Technology Review and Slashdot). Dr. Serre is the Faculty Director of the Center for Computation and Visualization and the Associate Director of the Initiative for Computation in Brain and Mind at Brown University. He also holds an International Chair in AI within the Artificial and Natural Intelligence Toulouse Institute (France). Dr. Serre has served as an area chair and a senior program committee member for top-tier machine learning and computer vision conferences including AAAI, CVPR, and NeurIPS. He is currently serving as a domain expert for IARPA’s Machine Intelligence from Cortical Networks (MICrONS) program and as a scientific advisor for Vium, Inc. He was the recipient of an NSF Early Career Award as well as DARPA’s Young Faculty Award and Director’s Award.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
