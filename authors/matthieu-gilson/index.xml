<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matthieu Gilson | CONECT | Computational Neuroscience Center @ INT</title>
    <link>https://conect-int.github.io/authors/matthieu-gilson/</link>
      <atom:link href="https://conect-int.github.io/authors/matthieu-gilson/index.xml" rel="self" type="application/rss+xml" />
    <description>Matthieu Gilson</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 22 Sep 2023 11:00:00 +0000</lastBuildDate>
    <image>
      <url>https://conect-int.github.io/authors/matthieu-gilson/avatar_hu649687aaa067e9fe98cb8aa085fbad98_57526_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Matthieu Gilson</title>
      <link>https://conect-int.github.io/authors/matthieu-gilson/</link>
    </image>
    
    <item>
      <title>Actors of CONECT</title>
      <link>https://conect-int.github.io/post/actors-conect/</link>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/post/actors-conect/</guid>
      <description>&lt;p&gt;Within the INT, many components of CONECT already exist, either carried by researchers in computational neurosciences or as themes strongly anchored in this field. A survey of the current situation reveals the existence of projects at different scales.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://conect-int.github.io/contact&#34;&gt;Contact&lt;/a&gt; us to be added!&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;from the cellular to the network level
&lt;ul&gt;
&lt;li&gt;deciphering the biophysical principles underlying robustness of neuronal activity using quantitative genotype-to-phenotype mapping strategies and realistic neuronal model databases (&lt;strong&gt;Jean-Marc Goaillard&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;dynamics and function of small and large-scale neural networks: &lt;strong&gt;&lt;a href=&#34;https://conect-int.github.io/authors/laurent-u-perrinet/&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/strong&gt; with &lt;a href=&#34;../../author/frederic-y-chavane&#34;&gt;Frédéric Chavane&lt;/a&gt;, &lt;strong&gt;&lt;a href=&#34;https://conect-int.github.io/authors/matthieu-gilson/&#34;&gt;Matthieu Gilson&lt;/a&gt;)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;from networks to mesoscopic levels :
&lt;ul&gt;
&lt;li&gt;Bayesian inference and predictive process models (&lt;strong&gt;&lt;a href=&#34;../../author/anna-montagnini&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/strong&gt;, &lt;a href=&#34;../../author/emmanuel-dauce&#34;&gt;Emmanuel Daucé&lt;/a&gt; and &lt;a href=&#34;https://conect-int.github.io/authors/laurent-u-perrinet/&#34;&gt;Laurent Perrinet&lt;/a&gt;), reinforcement learning, action selection, decision &lt;a href=&#34;../../author/andrea-brovelli&#34;&gt;Andrea Brovelli&lt;/a&gt; and &lt;a href=&#34;../../author/emmanuel-dauce&#34;&gt;Emmanuel Daucé&lt;/a&gt;), link with attentional mechanisms (Guilhem Ibos)&lt;/li&gt;
&lt;li&gt;information theory and functional connectivity for the analysis of cognitive brain networks (&lt;strong&gt;&lt;a href=&#34;../../author/andrea-brovelli&#34;&gt;Andrea Brovelli&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&#34;https://conect-int.github.io/authors/matthieu-gilson/&#34;&gt;Matthieu Gilson&lt;/a&gt;)&lt;/strong&gt; and &lt;a href=&#34;../../author/bruno-giordano&#34;&gt;Bruno Giordano&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;deep learning for data processing (&lt;strong&gt;&lt;a href=&#34;../../author/bruno-giordano&#34;&gt;Bruno Giordano&lt;/a&gt;&lt;/strong&gt;), deep learning + neuroimaging (&lt;em&gt;in voice perception&lt;/em&gt;) (Charly Lamothe) computational neuroscience and data processing in neuroinformatics (Sylvain Takerkart, NIT platform)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;at brain level
&lt;ul&gt;
&lt;li&gt;brain anatomy, particularly as applied to the formation of cortical folding (Julien Lefèvre with Guillaume Auzias, Sylvain Takerkart and &lt;strong&gt;&lt;a href=&#34;../../author/olivier-coulon&#34;&gt;Olivier Coulon&lt;/a&gt;&lt;/strong&gt;),&lt;/li&gt;
&lt;li&gt;the development of prognostic models of the evolution of certain pathologies (Lionel Velly, &lt;strong&gt;Sylvain Takerkart&lt;/strong&gt;),&lt;/li&gt;
&lt;li&gt;develop the collaboration of theoretical neurosciences with neuroinformatics, notably with the &lt;a href=&#34;http://www.int.univ-amu.fr/spip.php?page=plateform&amp;amp;equipe=CRISE&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NIT&lt;/a&gt; (&lt;strong&gt;Sylvain Takerkart&lt;/strong&gt;, Guillaume Auzias)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A structuring of these different components through a center (independent of existing and future teams) would be a major asset to reach a new stage in the creation of INT³.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Objectives of CONECT</title>
      <link>https://conect-int.github.io/post/objectives-conect/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/post/objectives-conect/</guid>
      <description>&lt;p&gt;The Computational Neuroscience Center (CONECT) is an incubator within the INT to promote theoretical and computational neuroscience.&lt;/p&gt;
&lt;p&gt;It ambitions to contribute to the training of a new generation of neuroscientists, following the revolution experienced by neuroscience over the past decades: tremendous technological advances across several disciplines have dramatically expanded the frontiers of experimentally accessible neuro-scientific facts. CONECT is thus concerned with new analysis tools and models that can account for large and complex datasets, in parallel with the NeuroTech Center that focuses on experimental devices.&lt;/p&gt;
&lt;p&gt;CONECT aims to build an inter-disciplinary community within the INT to foster interactions between computational neuroscientists and with experimentalists. The tools include scientific animation (journal clubs, seminars) and practical sessions to leanr and master new tools. This will participate in structuring the teaching and research environment around computational neuroscience around AMU beyond INT alone. The plan is to involve not only local research partners of INT like NeuroMarseille and the Laennec Institute, but also engineer schools (PolyTech, Centrale Marseille) and applied mathematics masters.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>2023-09-22: Coding Club x CONECT on Optuna</title>
      <link>https://conect-int.github.io/talk/2023-09-22-coding-club-x-conect-on-optuna/</link>
      <pubDate>Fri, 22 Sep 2023 11:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-09-22-coding-club-x-conect-on-optuna/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: On Fridays at &lt;em&gt;&lt;strong&gt;11.00am to 12.00am&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle Laurent Vinay (INT)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the first occurrence of monthly sessions on advanced computational tools for neuroscience to occur during the &lt;a href=&#34;https://framateam.org/int-marseille/channels/coding-club&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coding club&lt;/a&gt; organized by the &lt;a href=&#34;https://www.int.univ-amu.fr/plateformes/nit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NIT&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This Friday 22th Sept, we will have a look into &lt;a href=&#34;https://optuna.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt;, which is a generic-purpose optimizer for hyperparameters. It combines a diversity of &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modern search algorithms&lt;/a&gt; to optimize models in a black-box fashion, namely defining hyperparameters to optimize and an objective/cost function. Check the &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorials&lt;/a&gt; for further information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;COME TO SHARE AND TEST RECENT COMPUTATIONAL TOOLS FOR NEUROMODELING, ANALYZING YOUR DATA AND MUCH MORE!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The general purpose of these sessions is in line with the Coding Club on promoting open science in our daily practice. The focus of the CONECT contributions is on advanced (e.g. python, R, etc.) computational packages of interest for the local neuroscientific community, both theoreticians and experimentalists. Methodological questions like how to model specific neuronal systems and fitting experimental data are also in the scope of these sessions, with again a focus on practical tools related to simulation and analysis.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    To CONECT members: Propose topics at &lt;a href=&#34;https://amubox.univ-amu.fr/f/1359219652&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coding Club x CONECT&lt;/a&gt;.
And check the Mattermost channel for the schedule.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Monthly coding club by CONECT</title>
      <link>https://conect-int.github.io/slides/2023-09-22-coding-club-conect/</link>
      <pubDate>Fri, 22 Sep 2023 11:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/slides/2023-09-22-coding-club-conect/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/open-book.jpg&#34;
  &gt;

&lt;h1 id=&#34;monthly-coding-club-by-conect&#34;&gt;Monthly coding club by CONECT&lt;/h1&gt;
&lt;h2 id=&#34;advanced-computational-tools-for-neuroscience-and-open-science&#34;&gt;Advanced computational tools for neuroscience and open science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;When? on Friday per month, at 11.00am&lt;/li&gt;
&lt;li&gt;Where? salle Vinay at INT&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2023-09-22-at-1100am-optuna&#34;&gt;2023-09-22 at 11.00am: Optuna&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://optuna.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt; is a generic-purpose optimizer for hyperparameters. It combines a diversity of &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modern search algorithms&lt;/a&gt; to optimize models in a black-box fashion, namely defining hyperparameters to optimize and an objective/cost function. Check the &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorials&lt;/a&gt; for further information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2023-09-11: CONECT seminar by Taro Toyoizumi</title>
      <link>https://conect-int.github.io/talk/2023-09-11-conect-seminar-by-taro-toyoizumi/</link>
      <pubDate>Mon, 11 Sep 2023 14:30:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-09-11-conect-seminar-by-taro-toyoizumi/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;When: Monday 11th Sept 2023 &lt;em&gt;&lt;strong&gt;2.30pm to 3.30pm&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Where: &lt;em&gt;salle de cours 5, batiment principal Timone- (aile verte)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During this CONECT seminar co-organized with Institut de Neurosciences des Systèmes (INS), &lt;a href=&#34;https://toyoizumilab.riken.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taro Toyoizumi&lt;/a&gt; will present his recent work on &amp;ldquo;&lt;strong&gt;Modeling the fluctuations and state-dependence of synaptic dynamics&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstract: Adaptive behavior, crucial for thriving in complex environments, is believed to be enabled by activity-dependent synaptic plasticity within neural circuits. In the first part of this talk, I present how synaptic plasticity could be stabilized in the brain. Conventional models of Hebbian plasticity often facilitate connections between coincidentally active neurons and produce pathologically synchronous neural activity. I demonstrate that biologically observed intrinsic synaptic dynamics—activity-independent changes in synapses—can maintain a physiological distribution of synaptic strength and stabilize memory within neural networks. In the second part, I adopt a top-down approach to model synaptic plasticity. Viewing the brain as an efficient information-processing organ, I assume that synaptic weights are updated to transmit information between neurons efficiently. This theory provides insights into the distinct outcomes of synaptic plasticity observed during the up and down states of non-rapid eye movement sleep, thereby shedding light on how memory consolidation may be influenced by the states and spatial scale of slow waves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Taro Toyoizumi leads the Lab for Neural Computation and Adaptation at &lt;a href=&#34;https://cbs.riken.jp/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RIKEN Center for Brain Science&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2023-06-12: CONECT Workshop on Learning</title>
      <link>https://conect-int.github.io/talk/2023-06-12-conect-workshop-on-learning/</link>
      <pubDate>Mon, 12 Jun 2023 09:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2023-06-12-conect-workshop-on-learning/</guid>
      <description>&lt;h1 id=&#34;conect-workshop-active-learning-in-brains-and-machines&#34;&gt;CONECT Workshop &lt;em&gt;Active learning in brains and machines&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;We organized a one-day workshop on Monday, June 12th, at the Institut de Neurosciences de la Timone (INT), in the Gastaut room (9h-17h), campus Timone. The aim of CONECT one-day workshops was to gather computational neuroscientists and experimentalists around an open question in the field, with plenty of room for interaction and discussion.&lt;/p&gt;
&lt;p&gt;When? &lt;em&gt;12th of June 2023&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Where? &lt;a href=&#34;https://goo.gl/maps/MLpmsN9cd2N1Uv1L7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Campus Timone (room Gastaut), Aix-Marseille Université&lt;/em&gt;, 27 Boulevard Jean Moulin, 13005 Marseille&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Organizers: Simon Nougaret, Emmanuel Daucé, Laurent Perrinet (mobile: 0619478120), Matthieu Gilson&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;attendees&#34; srcset=&#34;
               /talk/2023-06-12-conect-workshop-on-learning/IMG_1617_hu55ebe531ef629f2f9bbb055848de5bda_1416425_905e1d9d1d818029cf8732850e05ba9b.webp 400w,
               /talk/2023-06-12-conect-workshop-on-learning/IMG_1617_hu55ebe531ef629f2f9bbb055848de5bda_1416425_7d552a93f2d9086f7a07f05ed8a2a7af.webp 760w,
               /talk/2023-06-12-conect-workshop-on-learning/IMG_1617_hu55ebe531ef629f2f9bbb055848de5bda_1416425_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://conect-int.github.io/talk/2023-06-12-conect-workshop-on-learning/IMG_1617_hu55ebe531ef629f2f9bbb055848de5bda_1416425_905e1d9d1d818029cf8732850e05ba9b.webp&#34;
               width=&#34;570&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;In biology, a major trait of neural systems is the capability to &lt;em&gt;learn&lt;/em&gt;, that is, to adapt its behavior to the environment it interacts with. Recent advances in machine learning and deep learning have, in parallel, contributed to formulate learning in terms of optimizing performance under task-specific domains.&lt;/p&gt;
&lt;p&gt;While each field inspires the other, there is still a gap in our understanding of how learning in machines may compare or even relate to learning in biology. The goal of this workshop is to allow people from both computational and experimental sides to understand current research achievements and challenges about &lt;em&gt;active learning in brains and machines&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;program&#34;&gt;PROGRAM&lt;/h3&gt;
&lt;p&gt;9:00 : Welcome &amp;amp; Introduction&lt;/p&gt;
&lt;h4 id=&#34;session-1--encoding-of-neuronal-representations-chair-emmanuel-daucé&#34;&gt;Session 1 : Encoding of neuronal representations (chair: Emmanuel Daucé)&lt;/h4&gt;
&lt;p&gt;9:15 : &lt;strong&gt;&lt;strong&gt;Alexandre Pitti&lt;/strong&gt;&lt;/strong&gt; (ETIS, CY-U, Cergy Pontoise): &amp;ldquo;Neuro-inspired mechanisms for sensorimotor and syntactic learning in language&amp;rdquo;&lt;/p&gt;
&lt;p&gt;9:55 : &lt;strong&gt;Laurie Mifsud &amp;amp; Matthieu Gilson&lt;/strong&gt; (INT, Marseille) &amp;ldquo;Statistical learning in bio-inspired neuronal network&amp;rdquo;&lt;/p&gt;
&lt;p&gt;10:15 : &lt;strong&gt;Antoine Grimaldi&lt;/strong&gt; (INT, Marseille) &amp;ldquo;Learning in networks of spiking neurons with heterogeneous delays&amp;rdquo;&lt;/p&gt;
&lt;p&gt;10:35 : coffee break&lt;/p&gt;
&lt;h4 id=&#34;session-2--learning-action-selection-chair-matthieu-gilson&#34;&gt;Session 2 : Learning action selection (chair: Matthieu Gilson)&lt;/h4&gt;
&lt;p&gt;11:00 : &lt;strong&gt;&lt;strong&gt;Jorge Ramirez Ruiz&lt;/strong&gt;&lt;/strong&gt; (Univ Pompeu Fabra, Barcelona) &amp;ldquo;Path occcupancy maximization principle&amp;rdquo;&lt;/p&gt;
&lt;p&gt;11:40 : &lt;strong&gt;Nicolas Meirhaeghe&lt;/strong&gt; (INT, Marseille) : &amp;ldquo;Bayesian Computation through Cortical Latent Dynamics&amp;rdquo;&lt;/p&gt;
&lt;p&gt;12:00 : &lt;strong&gt;Emmanuel Daucé &amp;amp; Hamza Oueld&lt;/strong&gt; (INT, Marseille) : &amp;ldquo;Principles of model-driven active sampling in the brain&amp;rdquo;&lt;/p&gt;
&lt;p&gt;12:30 : Meal&lt;/p&gt;
&lt;h4 id=&#34;session-3--neuronal-basis-of-vocal-representation-chair-laurent-perrinet&#34;&gt;Session 3 : Neuronal basis of vocal representation (chair: Laurent Perrinet)&lt;/h4&gt;
&lt;p&gt;14:00 : &lt;strong&gt;&lt;strong&gt;Thomas Schatz&lt;/strong&gt;&lt;/strong&gt; (LIS, Marseille): &amp;ldquo;Perceptual development, unsupervised representation learning and auditory neuroscience&amp;rdquo;&lt;/p&gt;
&lt;p&gt;14:40 : &lt;strong&gt;Charly Lamothe&lt;/strong&gt; (LIS/INT, Marseille) &amp;amp; &lt;strong&gt;Etienne Thoret&lt;/strong&gt; (PRISM/LIS/ILCB, Marseille): &amp;ldquo;Decoding voice identity from brain activity&amp;rdquo;&lt;/p&gt;
&lt;p&gt;15:00 : coffee break&lt;/p&gt;
&lt;h4 id=&#34;session-4--distribution-and-integration-of-brain-functions-chair-simon-nougaret&#34;&gt;Session 4 : Distribution and integration of brain functions (chair: Simon Nougaret)&lt;/h4&gt;
&lt;p&gt;15:30 : &lt;strong&gt;&lt;strong&gt;Jean-Rémi King&lt;/strong&gt;&lt;/strong&gt; (Meta / CNRS): &amp;ldquo;Language in the brain and algorithms&amp;rdquo;&lt;/p&gt;
&lt;p&gt;16:10 : &lt;strong&gt;Etienne Combrisson &amp;amp; Andrea Brovelli&lt;/strong&gt; (INT, Marseille) : &amp;ldquo;Cortico-cortical interactions for goal-directed causal learning&amp;rdquo;&lt;/p&gt;
&lt;p&gt;16:30 : Round table&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;attendees&#34; srcset=&#34;
               /talk/2023-06-12-conect-workshop-on-learning/IMG_7807_hua238fd24969325e44a2e5a20c16ddb0f_1676835_4213d5016f5c6256ba9380ce4d3e75b3.webp 400w,
               /talk/2023-06-12-conect-workshop-on-learning/IMG_7807_hua238fd24969325e44a2e5a20c16ddb0f_1676835_c1ac7a60351ca73a881e363dbcf38d44.webp 760w,
               /talk/2023-06-12-conect-workshop-on-learning/IMG_7807_hua238fd24969325e44a2e5a20c16ddb0f_1676835_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://conect-int.github.io/talk/2023-06-12-conect-workshop-on-learning/IMG_7807_hua238fd24969325e44a2e5a20c16ddb0f_1676835_4213d5016f5c6256ba9380ce4d3e75b3.webp&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstracts&#34;&gt;Abstracts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Andrea Brovelli: &amp;ldquo;Cortico-cortical interactions for goal-directed learning&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;During my presentation, I will provide a concise overview of two recent studies that explore the significance of cortico-cortical interactions in goal-directed learning and the processing of outcome-related learning computations, specifically prediction errors. In the first study, we examined the interaction between human prefrontal and insular regions during reward and punishment learning. Using intracranial EEG recordings to measure high-gamma activity (HGA) and leveraging advancements in information theory, we discovered a functional distinction in inter-areal interactions between reward and punishment learning. A reward subsystem with redundant interactions was observed between the orbitofrontal and ventromedial prefrontal cortices, where the ventromedial prefrontal cortex played a Granger-causality driving role. Additionally, we identified a punishment subsystem with redundant interactions between the insular and dorsolateral cortices, with the insula acting as the primary driver. Furthermore, we found that the encoding of both reward and punishment prediction errors was mediated by synergistic interactions between these two subsystems. In the second study, we investigated the spatio-temporal characteristics of cortico-cortical interactions that support learning-related variables, such as reward-related signals (Bayesian surprise). Our results revealed the involvement of a distributed network comprising the visual, lateral prefrontal, and orbitofrontal cortex. Preliminary findings also indicated the presence of higher-order synergistic interactions that emerge from the combined activation of these networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Emmanuel Daucé &amp;amp; Hamza Oueld : &amp;ldquo;Principles of model-driven active sampling in the brain&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Understanding our environment requires not only passively observing sensory samples, but also acting to seek out useful relationships between our actions and their possible outcomes. Inspired by the concept of &amp;ldquo;visual salience&amp;rdquo;, we provide a way to interpret action selection as making an &amp;ldquo;ideal experiment&amp;rdquo;, in a behavioral task where participants estimate the causal influence of a player on the outcome of a volleyball game. We show that the balance between the accuracy and the diversity objectives can lead to specific action selection biases, reflected both in the model and in the data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Antoine Grimaldi: &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Learning in networks of spiking neurons with heterogeneous delays&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neural code is essential to understand information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. However, most artificial neural models do not take advantage of this important temporal dimension of the neural code. Inspired by this neuroscientific observation, we develop a model for the efficient detection of temporal spiking motifs based on a layer of spiking neurons with heterogeneous synaptic delays. The model uses the property that the diversity of synaptic delays on the dendritic tree allows for the synchronization of specific arrangements of synaptic inputs as they reach the basal dendritic tree. We show that this can be formalized as a time-invariant logistic regression that can be trained on labeled data. We demonstrate its application to synthetic naturalistic videos transformed into event streams similar to the output of the retina or to event-based cameras and for which we will characterize the accuracy of the model in detecting visual motion. In particular, we quantify how the accuracy can vary as a function of the overall computational load showing it is still efficient at very low workloads. This end-to-end, event-driven computational building block could improve the performance of future spiking neural network (SNN) algorithms and in particular their implementation in neuromorphic chips.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Charly Lamothe &amp;amp; Etienne Thoret: &amp;ldquo;Decoding voice identity from brain activity&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Voice information processing in the brain involves specialized areas called temporal voice areas (TVAs), which respond more strongly to vocalizations from the same species. However, how these areas represent voice information, specifically speaker identity, is not well understood. To investigate this, we used a deep neural network (DNN) to create a compact representation of voice stimuli called the voice latent space (VLS). We then examined the relationship between the VLS and brain activity using various analyses. We discovered that the VLS correlates with cerebral activity measured by fMRI when exposed to thousands of voice stimuli from numerous speakers. The VLS also better captures the representation of speaker identity in the TVAs compared to the primary auditory cortex (A1). Additionally, the VLS enables reconstructions of voice stimuli in the TVAs that maintain important aspects of speaker identity, as confirmed by both machine classifiers and human listeners. These findings suggest that the DNN-derived VLS provides high-level representations of voice identity information in the TVAs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Jean-Rémi King: &amp;ldquo;Language in the brain and algorithms.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Deep learning has recently made remarkable progress in natural language processing. Yet, the resulting algorithms fall short of the efficiency of the human brain. To bridge this gap, we here explore the similarities and differences between these two systems using large-scale datasets of magneto/electro-encephalography (M/EEG), functional Magnetic Resonance Imaging (fMRI), and intracranial recordings. After investigating where and when deep language algorithms map onto the brain, we show that enhancing these algorithms with long-range forecasts makes them more similar to the brain. Our results further reveal that, unlike current deep language models, the human brain is tuned to generate a hierarchy of long-range predictions, whereby the fronto-parietal cortices forecast more abstract and more distant representations than the temporal cortices. Overall, our studies show how the interface between AI and neuroscience clarifies the computational bases of natural language processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Nicolas Meirhaerghe: &amp;ldquo;Neural correlate of prior expectations in timing behavior&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;A central function of the brain is the capacity to anticipate the timing of future events based on past experience. Picture, for instance, a naive baseball player attempting to intercept an incoming ball. After a few failed trials, the player becomes increasingly close to striking the ball, and ultimately hits a home run. How does information about the past few throws help guide the timing of the player’s action? In this talk, I will present behavioral and electrophysiological data from non-human primates aimed at understanding how temporal expectations are represented in the brain. Specifically, I will address the following two questions: (1) how prior knowledge about the timing of a future event is encoded at the level of single neurons, and induces systematic biases in behavior; (2) what type of neural and behavioral changes occur when temporal expectations change in a dynamic environment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Laurie Mifsud &amp;amp; Matthieu Gilson: &amp;ldquo;Statistical learning in bio-inspired neuronal network&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In biological neuronal networks, information representation and processing like learning by synaptic plasticity rules are not only related to first-order statistics (i.e. mean firing rate) but also second and higher-order statistics in spike trains. This palces the focus on the temporal structure of distributed spiking activity, at several timescales. In parallel, recent models in machine learning like deep temporal convolutional networks have switched from inputs like static images to time series. In both cases, the goal is to extract spatio-temporal patterns of activity and it can be framed in the context of statistical learning. We will start from experimental evidence about spiking activity during a cognitive task, then present recent work combining covariance-based learning and reservoir computing to classify time series. The results highlight the important role for the recurrent connectivity in transforming information representations in biologically inspired architectures. Finally, we will see how to use this supervised learning framework to tune a recurrently connected population to experimental spiking data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Alex Pitti : &amp;ldquo;Neuro-inspired mechanisms for sensorimotor and syntactic learning in language&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;I propose that two neural mechanisms are at work for language acquisition. On the one hand, predictive coding and on the other hand, serial order coding. Predictive coding helps connect causes to effects in sensorimotor coordination during voice learning. While serial order codes allow pattern recognition in sentences to extract syntactic rules and hierarchy. The coupling between two neural architectures based on these two mechanisms, resp. the cortico-basal system and the fronto-striatum system can be used for the acquisition and categorization of sound primitives (syllables) and sequences (words). As a surprising extension of this idea, we have found that serial codes produce as well efficient coding and can reach Shannon&amp;rsquo;s limit in terms of information capacity. Langage is a compressive representation of information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Jorge Ramirez Ruiz: &amp;ldquo;A maximum occupancy principle for brains and behavior&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The usual approach to analyze naturalistic behavior is to define its function as some form of reward or utility maximization. However, inferring the reward function for natural agents or designing one for artificial ones is problematic due to the unobservability of internal states and to the appearance of unintended behavior, respectively. Here, we abandon the idea of reward maximization and propose a principle of behavior based on the intrinsic motivation to maximize the occupancy of future action and state paths. We reconceptualize ‘reward’ as means to occupy paths, instead of the goals. We show that goal-directed behavior emerges from this principle by applying it to various discrete and continuous state tasks. In particular, we can apply this principle to a network of recurrently connected neurons, and we show that it is possible to produce highly variable activity while avoiding the saturation of the units. This work provides a proof of concept that goal-directedness is possible in the complete absence of external reward maximization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Thomas Schatz: &amp;ldquo;Perceptual development, unsupervised representation learning and auditory neuroscience&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;I will draw from ongoing research projects at the interface between developmental psychology, machine learning, and computational neuroscience, to illustrate how, in my view, perspectives from each of these fields may contribute to the others. More specifically, I will discuss how considerations from developmental psychology and computational neuroscience can inform the design of novel algorithms for the unsupervised learning of speech representations and how the study of these algorithms may, in turn, lead to a deeper understanding of dynamic signal processing in the human brain and of perceptual development in infancy.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2022-11-28: CONECT at the INT brainhack: Automatic detection of spiking motifs in neurobiological data</title>
      <link>https://conect-int.github.io/talk/2022-11-28-conect-at-the-int-brainhack-automatic-detection-of-spiking-motifs-in-neurobiological-data/</link>
      <pubDate>Mon, 28 Nov 2022 09:00:00 +0000</pubDate>
      <guid>https://conect-int.github.io/talk/2022-11-28-conect-at-the-int-brainhack-automatic-detection-of-spiking-motifs-in-neurobiological-data/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;TL;DR This project aims to develop a method for the automated detection of repeating spiking motifs, possibly noisy, in ongoing activity. Results are available on the shared repo: &lt;a href=&#34;https://github.com/SpikeAI/2022-11_brainhack_DetecSpikMotifs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SpikeAI/2022-11_brainhack_DetecSpikMotifs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mattermost.brainhack.org/brainhack/channels/bhg22-marseille-detecspikmotifs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mattermost channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;h3 id=&#34;leaders&#34;&gt;Leaders&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://matthieugilson.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matthieu Gilson&lt;/a&gt; - &lt;a href=&#34;https://github.com/MatthieuGilson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/MatthieuGilson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt; - &lt;a href=&#34;https://github.com/LaurentPerrinet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/LaurentPerrinet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;collaborators&#34;&gt;Collaborators&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hugo Ladret&lt;/li&gt;
&lt;li&gt;George Abitbol&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;brainhack-global-2022-event&#34;&gt;Brainhack Global 2022 Event&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://brainhack-marseille.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brainhack Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;supported by the &lt;a href=&#34;https://laurentperrinet.github.io/grant/polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Polychronies&lt;/a&gt; grant&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;project-description&#34;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;The study of spatio-temporal correlated activity patterns is very active in several fields related to neuroscience, like machine learning in vision &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/29563572/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Muller Nat Rev Neurosci 2018)&lt;/a&gt; and neuronal representations and processing &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/31110324/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Shahidi Nat Neurosci 2019)&lt;/a&gt;. &lt;strong&gt;This project aims to develop a method for the automated detection of repeating spiking motifs, possibly noisy, in ongoing activity.&lt;/strong&gt; A diversity of formalizations and detection methods have been proposed and we will focus on several example measures for event/spike trains, to be compared on both synthetic and real data.&lt;/p&gt;
&lt;p&gt;An implementation could be based on autodifferentiable networks as implemented in Python libraries like pytorch. This framework allows for the tuning of parameters with specific architectures like convolutional layers that can capture various timescales in spike patterns (e.g. latencies) in an automated fashion. Another recent tool based on the estimation of firing probability for a range of latencies has been proposed &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/grimaldi-23-bc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Grimaldi ICIP 2022)&lt;/a&gt;. This will be compared with existing approaches like Elephant’s &lt;a href=&#34;https://elephant.readthedocs.io/en/latest/reference/spade.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPADE&lt;/a&gt; or decoding techniques based on computed statistics computed on smoothed spike trains (adapted from time series processing, see &lt;a href=&#34;https://doi.org/10.1101/2021.04.30.441789&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Lawrie, biorxiv&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;One part concerns the generation of realistic synthetic data producing spike trains  which include spiking motifs with specific latencies or comodulation of firing rate. The goal is to test how these different structures, which rely on specific assumptions about e.g. stationarity or independent firing probability across time, can be captured by different detection methods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
